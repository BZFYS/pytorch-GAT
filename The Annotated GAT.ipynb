{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Annotated GAT\n",
    "\n",
    "The idea of this notebook is to make it easier even for non-researchers to understand the Graph Attention Network (and GNNs in general)!\n",
    "\n",
    "In this notebook you'll get answers to these questions:\n",
    "\n",
    "‚úÖ What is GAT exactly? <br/>\n",
    "‚úÖ How to train it? <br/>\n",
    "‚úÖ How to use it (Cora classification example)? <br/>\n",
    "\n",
    "After you complete this one you'll have a much better understanding of graph neural networks in general!\n",
    "\n",
    "*Note: Cora is a transductive setting, I'll be adding an inductive example soon as well (PPI - protein protein interaction, probably).*\n",
    "\n",
    "Nice, let's start!\n",
    "\n",
    "---\n",
    "\n",
    "## What the heck are Graph Attention Networks?\n",
    "\n",
    "Graph Attention Network, or GAT for short, is a Graph Neural Network (GNN) published by `Veliƒçkoviƒá et al.` in a paper called [Graph Attention Networks](https://arxiv.org/abs/1710.10903) back in 2017.\n",
    "\n",
    "It turns out that combining the idea of **attention** with the already existing **graphs convolutional networks** (GCN) was a good move ü§ì - GAT is the **2nd most cited** paper in the GNN literature (as of the time of writing this).\n",
    "\n",
    "So because `GCN + attention = GAT` in order to understand GAT you basically need to understand GCNs.\n",
    "\n",
    "The whole idea came from CNNs (*stack push GCN (a nervous chuckle)*). Convolutional Neural Networks were working so nicely, solving various computer vision tasks and creating a huge hype in the world of deep learning, so some folks decided to transfer the idea onto graphs.\n",
    "\n",
    "The basic problem is that while the image lies on a regular grid (which you can also treat as a graph (*sighs*)), and thus has a precise notion of **order** (e.g. my **top-left** neighboor (*popularly known as pixels in the CV world*)), graphs don't enjoy that nice property and both the number of neighbors as well as the order of neighbors may vary. \n",
    "\n",
    "How can you define a kernel for a graph? The kernel size can't be `3x3` because sometimes a node will have 2 neighbors and sometimes 233240 (*breaks the keyboard*).\n",
    "\n",
    "2 main ideas popped up:\n",
    "* **spectral methods** (they all somehow leverage the graph Laplacian eigenbasis (I'll completely ignore them here))\n",
    "* **spatial methods** \n",
    "\n",
    "Although spatial methods can vaguely be motivated by the spectral ones it's much more healthy to think of them directly from the spatial perspective. Ok, here it goes. (*stack pop GCN*)\n",
    "\n",
    "---\n",
    "\n",
    "**High level explanation of spatial (message passing) methods:** \n",
    "\n",
    "So you have the feature vectors from your neighbors at your disposal. You do the following:\n",
    "\n",
    "1. You somehow transform them (maybe a linear projection)\n",
    "2. You somehow aggregate them (maybe weighing them with attention coefficients, voil√†, we get GAT (*you see what I did there*))\n",
    "3. You update the feature vector (somehow) of the current node by combining it's (transformed) feature vector with the aggregated neighborhood representation.\n",
    "\n",
    "And that's pretty much it, you can fit many different GNNs into this framework.\n",
    "\n",
    "Here is how GAT schematic looks like (those differently colored edges represent different attention heads):\n",
    "\n",
    "<img src=\"data/readme_pics/GAT_schematic.PNG\" alt=\"transformer architecture\" align=\"center\" style=\"width: 500px;\"/> <br/>\n",
    "\n",
    "**Fun fact:** *transformers* can be thought of as a special case of *GAT* - when the input graph is **fully-connected**. Check out [this blog](https://thegradient.pub/transformers-are-graph-neural-networks/) for more details.\n",
    "\n",
    "---\n",
    "\n",
    "That was everything you need to know for now! <br/>\n",
    "\n",
    "If you need further help understanding all of the details I created this [in-depth overview of the GAT paper:](https://www.youtube.com/watch?v=uFLeKkXWq2c)\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=uFLeKkXWq2c\" target=\"_blank\"><img src=\"https://img.youtube.com/vi/uFLeKkXWq2c/0.jpg\" \n",
    "alt=\"An in-depth overview of the Graph Attention Networks\" width=\"480\" align=\"left\" height=\"360\" border=\"10\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note: the code in this notebook is a strict subset of the code that's at your disposal in this repository. I'll focus on a single GAT implementation here (the conceptually hardest one to understand, I have 3 imps actually!) and I'll omit some of the visulizations.**\n",
    "\n",
    "**If you want me to cover something additionally, open up a \"feature\" request issue.** ‚ù§Ô∏è\n",
    "\n",
    "With that out of the way let's dig in! Let's start with imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I always like to structure my imports into Python's native libs,\n",
    "# stuff I installed via conda/pip and local file imports (but we don't have those here)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Visualization related imports\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Main computation libraries\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "# Deep learning related imports\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Contains constants needed for data loading.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import enum\n",
    "\n",
    "\n",
    "# Supported datasets - currently only Cora\n",
    "class DatasetType(enum.Enum):\n",
    "    CORA = 0\n",
    "\n",
    "    \n",
    "# Networkx is not precisely made with drawing as it's main feature but I experimented with it a bit\n",
    "class GraphVisualizationTool(enum.Enum):\n",
    "    NETWORKX = 0,\n",
    "    IGRAPH = 1\n",
    "\n",
    "\n",
    "# We'll be dumping and reading the data from this directory\n",
    "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data')\n",
    "CORA_PATH = os.path.join(DATA_DIR_PATH, 'cora')  # this is checked-in no need to make a directory\n",
    "\n",
    "#\n",
    "# Cora specific constants\n",
    "#\n",
    "\n",
    "# Thomas Kipf et al. first used this split in GCN paper and later Petar Veliƒçkoviƒá et al. in GAT paper\n",
    "CORA_TRAIN_RANGE = [0, 140]  # we're using the first 140 nodes as the training nodes\n",
    "CORA_VAL_RANGE = [140, 140+500]\n",
    "CORA_TEST_RANGE = [1708, 1708+1000]\n",
    "CORA_NUM_INPUT_FEATURES = 1433\n",
    "CORA_NUM_CLASSES = 7\n",
    "\n",
    "# Used whenever we need to plot points from different class (like t-SNE in playground.py and CORA visualization)\n",
    "cora_label_to_color_map = {0: \"red\", 1: \"blue\", 2: \"green\", 3: \"orange\", 4: \"yellow\", 5: \"pink\", 6: \"gray\"}\n",
    "\n",
    "# check out this site has a nice visualization of Cora as well\n",
    "# r'http://networkrepository.com/graphvis.php?d=./data/gsm50/labeled/cora.edges'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Understanding your data (become One with the data ‚ù§Ô∏è)\n",
    "\n",
    "I'll be using Cora citation network as the running example and I'll probably add an inductive example soon (like protein-protein interactions (PPI) dataset).\n",
    "\n",
    "Having said that, you may wonder, what's the difference between `transductive` and `inductive` setting? If you're not familiar with GNNs this may appear as a weird concept. But it's quite simple.\n",
    "\n",
    "**Transductive** - you have a single graph (like Cora) you split some **nodes** (and not graphs) into train/val/test training sets. While you're training you'll be using only the labels for your training nodes. BUT. During the forward prop, by the nature of how spatial GNNs work, you'll be aggregating the feature vectors from your neighbors and **some of them may belong to val or even test sets!** The main point is - you **ARE NOT** using their label information but you **ARE** using the structural information and their features.\n",
    "\n",
    "**Inductive** - you're probably much more familiar with this one if you come from the computer vision or NLP background. You have a set of training graphs, a separate set of val graphs and of course a separate set of test graphs.\n",
    "\n",
    "Having explained that let's jump into the code and let's load and visualize Cora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's define these simple functios for loading/saving Pickle files - we need them for Cora\n",
    "\n",
    "# All Cora data is stored as pickle\n",
    "def pickle_read(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "def pickle_save(path, data):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can load Cora!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll pass the training config dictionary a bit later\n",
    "def load_graph_data(training_config, device):\n",
    "    dataset_name = training_config['dataset_name'].lower()\n",
    "    should_visualize = training_config['should_visualize']\n",
    "\n",
    "    if dataset_name == DatasetType.CORA.name.lower():\n",
    "\n",
    "        # shape = (N, FIN), where N is the number of nodes and FIN is the number of input features\n",
    "        node_features_csr = pickle_read(os.path.join(CORA_PATH, 'node_features.csr'))\n",
    "        # shape = (N, 1)\n",
    "        node_labels_npy = pickle_read(os.path.join(CORA_PATH, 'node_labels.npy'))\n",
    "        # shape = (N, number of neighboring nodes) <- this is a dictionary not a matrix!\n",
    "        adjacency_list_dict = pickle_read(os.path.join(CORA_PATH, 'adjacency_list.dict'))\n",
    "\n",
    "        # Normalize the features (helps with training)\n",
    "        node_features_csr = normalize_features_sparse(node_features_csr)\n",
    "        num_of_nodes = len(node_labels_npy)\n",
    "\n",
    "        # shape = (2, E), where E is the number of edges, and 2 for source and target nodes. Basically edge index\n",
    "        # contains tuples of the format S->T, e.g. 0->3 means that node with id 0 points to a node with id 3.\n",
    "        topology = build_edge_index(adjacency_list_dict, num_of_nodes, add_self_edges=True)\n",
    "\n",
    "        # Note: topology is just a fancy way of naming the graph structure data\n",
    "\n",
    "        if should_visualize:  # network analysis and graph drawing\n",
    "            plot_in_out_degree_distributions(topology, num_of_nodes, dataset_name)  # we'll define these in a second\n",
    "            visualize_graph(topology, node_labels_npy, dataset_name)\n",
    "\n",
    "        # Convert to dense PyTorch tensors\n",
    "\n",
    "        # Needs to be long int type because later functions like PyTorch's index_select expect it\n",
    "        topology = torch.tensor(topology, dtype=torch.long, device=device)\n",
    "        node_labels = torch.tensor(node_labels_npy, dtype=torch.long, device=device)  # Cross entropy expects a long int\n",
    "        node_features = torch.tensor(node_features_csr.todense(), device=device)\n",
    "\n",
    "        # Indices that help us extract nodes that belong to the train/val and test splits\n",
    "        train_indices = torch.arange(CORA_TRAIN_RANGE[0], CORA_TRAIN_RANGE[1], dtype=torch.long, device=device)\n",
    "        val_indices = torch.arange(CORA_VAL_RANGE[0], CORA_VAL_RANGE[1], dtype=torch.long, device=device)\n",
    "        test_indices = torch.arange(CORA_TEST_RANGE[0], CORA_TEST_RANGE[1], dtype=torch.long, device=device)\n",
    "\n",
    "        return node_features, node_labels, topology, train_indices, val_indices, test_indices\n",
    "    else:\n",
    "        raise Exception(f'{dataset_name} not yet supported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, there are 2 more functions that I've used that we're yet to define. First let's see how we do feature normalization on Cora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features_sparse(node_features_sparse):\n",
    "    assert sp.issparse(node_features_sparse), f'Expected a sparse matrix, got {node_features_sparse}.'\n",
    "\n",
    "    # Instead of dividing (like in normalize_features_dense()) we do multiplication with inverse sum of features.\n",
    "    # Modern hardware (GPUs, TPUs, ASICs) is optimized for fast matrix multiplications! ^^ (* >> /)\n",
    "    # shape = (N, FIN) -> (N, 1), where N number of nodes and FIN number of input features\n",
    "    node_features_sum = np.array(node_features_sparse.sum(-1))  # sum features for every node feature vector\n",
    "\n",
    "    # Make an inverse (remember * by 1/x is better (faster) then / by x)\n",
    "    # shape = (N, 1) -> (N)\n",
    "    node_features_inv_sum = np.power(node_features_sum, -1).squeeze()\n",
    "\n",
    "    # Again certain sums will be 0 so 1/0 will give us inf so we replace those by 1 which is a neutral element for mul\n",
    "    node_features_inv_sum[np.isinf(node_features_inv_sum)] = 1.\n",
    "\n",
    "    # Create a diagonal matrix whose values on the diagonal come from node_features_inv_sum\n",
    "    diagonal_inv_features_sum_matrix = sp.diags(node_features_inv_sum)\n",
    "\n",
    "    # We return the normalized features.\n",
    "    return diagonal_inv_features_sum_matrix.dot(node_features_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's basically making Cora's binary node feature vectors sum up to 1. Example if we had `[1, 0, 1, 0, 1]` (Cora's feature vectors are longer as we'll soon see but let's take this one for the time being), it will get transformed into `[0.33, 0, 0.33, 0, 0.33]`. Simple as that. It's always harder to understand the actual implementation but conceptually it's a piece of cake.\n",
    "\n",
    "That out of the way let's build up that edge index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edge_index(adjacency_list_dict, num_of_nodes, add_self_edges=True):\n",
    "    source_nodes_ids, target_nodes_ids = [], []\n",
    "    seen_edges = set()\n",
    "\n",
    "    for src_node, neighboring_nodes in adjacency_list_dict.items():\n",
    "        for trg_node in neighboring_nodes:\n",
    "            # if this edge hasn't been seen so far we add it to the edge index (coalescing - removing duplicates)\n",
    "            if (src_node, trg_node) not in seen_edges:  # it'd be easy to explicitly remove self-edges (Cora has none..)\n",
    "                source_nodes_ids.append(src_node)\n",
    "                target_nodes_ids.append(trg_node)\n",
    "\n",
    "                seen_edges.add((src_node, trg_node))\n",
    "\n",
    "    if add_self_edges:\n",
    "        source_nodes_ids.extend(np.arange(num_of_nodes))\n",
    "        target_nodes_ids.extend(np.arange(num_of_nodes))\n",
    "\n",
    "    # shape = (2, E), where E is the number of edges in the graph\n",
    "    edge_index = np.row_stack((source_nodes_ids, target_nodes_ids))\n",
    "\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one should be fairly simple - we just accumulate the edges in this format: <br/>\n",
    "[[0, 1], [2, 2], ...] where [s, t] tuple basically defines an edge where node `s` (source) points to node `t` (target).\n",
    "\n",
    "Nice, finally let's try and load it. We should also analyze the shapes - that's always a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 1433]) torch.float32\n",
      "torch.Size([2708]) torch.int64\n",
      "torch.Size([2, 13264]) torch.int64\n",
      "torch.Size([140]) torch.int64\n",
      "torch.Size([500]) torch.int64\n",
      "torch.Size([1000]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Let's just define dummy visualization functions for now - just to stop Python interpreter from complaining!\n",
    "# We'll define them in a moment, properly, I swear.\n",
    "\n",
    "def plot_in_out_degree_distributions():\n",
    "    pass\n",
    "\n",
    "def visualize_graph():\n",
    "    pass\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU\n",
    "\n",
    "config = {\n",
    "    'dataset_name': DatasetType.CORA.name,\n",
    "    'should_visualize': False\n",
    "}\n",
    "\n",
    "node_features, node_labels, edge_index, train_indices, val_indices, test_indices = load_graph_data(config, device)\n",
    "\n",
    "print(node_features.shape, node_features.dtype)\n",
    "print(node_labels.shape, node_labels.dtype)\n",
    "print(edge_index.shape, edge_index.dtype)\n",
    "print(train_indices.shape, train_indices.dtype)\n",
    "print(val_indices.shape, val_indices.dtype)\n",
    "print(test_indices.shape, test_indices.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Analyzing the shapes we see the following:\n",
    "1. Cora has 2708 nodes\n",
    "2. Each node has 1433 features (check out [data_loading.py](https://github.com/gordicaleksa/pytorch-GAT/blob/main/utils/data_loading.py) for much more detail)\n",
    "3. We have 13264 edges! (including the self edges)\n",
    "4. We have 140 training nodes\n",
    "5. We have 500 val nodes\n",
    "6. We have 1000 test nodes\n",
    "\n",
    "Additionally almost all of the data is of int 64 type. Why? Well it's a constraint that PyTorch is imposing upon us.\n",
    "The loss function `nn.CrossEntropyLoss` and `index_select` functions require torch.long (i.e. 64 bit integer) - that's it.\n",
    "\n",
    "* `node_labels` is int64 because of `nn.CrossEntropyLoss`\n",
    "* other vars are int64 because of `index_select`\n",
    "\n",
    "It's always a **good idea to test your code as you're progressing.** \n",
    "\n",
    "Data loading is completely orthogonal to the rest of this notebook so we can test it, standalone, and make sure the shapes and datatypes make sense. I use this strategy while developing projects like this one (and in general).\n",
    "\n",
    "I start with data I add the loading functionality, I add some visualizations and only then do I usually start developing the deep learning model itself.\n",
    "\n",
    "Visualizations are a huge bonus, so let's develop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
